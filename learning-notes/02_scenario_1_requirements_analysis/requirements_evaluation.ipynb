{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements Evaluation Framework\n",
    "\n",
    "**Purpose:** Systematic evaluation of prompt effectiveness for learning requirements gathering\n",
    "\n",
    "**Approach:** Data-driven measurement using concrete criteria, not subjective opinions\n",
    "\n",
    "**Goal:** Identify which prompt versions generate the most actionable learning requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Load the most recent test results\n",
    "def load_latest_results():\n",
    "    \"\"\"Load the most recent prompt test results\"\"\"\n",
    "    result_files = glob.glob('data/prompt_test_results_*.csv')\n",
    "    if not result_files:\n",
    "        print(\"‚ùå No test results found. Run the prompts notebook first.\")\n",
    "        return None\n",
    "    \n",
    "    # Get most recent file\n",
    "    latest_file = max(result_files, key=os.path.getctime)\n",
    "    print(f\"üìä Loading results from: {latest_file}\")\n",
    "    \n",
    "    df = pd.read_csv(latest_file)\n",
    "    print(f\"‚úÖ Loaded {len(df)} prompt test results\")\n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "results_df = load_latest_results()\n",
    "\n",
    "if results_df is not None:\n",
    "    print(\"\\nüìã Available prompt versions:\")\n",
    "    for version in results_df['version'].unique():\n",
    "        print(f\"- {version}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "Based on the learning scenario, we'll evaluate prompts on:\n",
    "\n",
    "1. **Specificity** - How specific and concrete are the requirements?\n",
    "2. **Actionability** - Can someone immediately act on these requirements?\n",
    "3. **Relevance** - How well do they serve the current learning scenario?\n",
    "4. **Structure** - How well organized and readable is the response?\n",
    "5. **Practicality** - How realistic are these for daily implementation?\n",
    "\n",
    "Each criterion scored 1-5 (5 being best), total possible score: 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_specificity(response):\n",
    "    \"\"\"Score 1-5 based on how specific and concrete the requirements are\"\"\"\n",
    "    # Count specific, actionable terms\n",
    "    specific_terms = len(re.findall(r'\\b(build|create|implement|practice|complete|learn|master|use)\\b', response.lower()))\n",
    "    concrete_terms = len(re.findall(r'\\b(jupyter|notebook|api|python|git|github|claude|anthropic)\\b', response.lower()))\n",
    "    \n",
    "    total_specific = specific_terms + concrete_terms\n",
    "    \n",
    "    if total_specific >= 15: return 5\n",
    "    elif total_specific >= 10: return 4\n",
    "    elif total_specific >= 6: return 3\n",
    "    elif total_specific >= 3: return 2\n",
    "    else: return 1\n",
    "\n",
    "def evaluate_actionability(response):\n",
    "    \"\"\"Score 1-5 based on how immediately actionable the requirements are\"\"\"\n",
    "    # Look for actionable language patterns\n",
    "    action_patterns = [\n",
    "        r'start by',\n",
    "        r'begin with',\n",
    "        r'first.*do',\n",
    "        r'step \\d+',\n",
    "        r'today.*can',\n",
    "        r'immediately',\n",
    "        r'next action'\n",
    "    ]\n",
    "    \n",
    "    action_count = sum(len(re.findall(pattern, response.lower())) for pattern in action_patterns)\n",
    "    \n",
    "    if action_count >= 5: return 5\n",
    "    elif action_count >= 3: return 4\n",
    "    elif action_count >= 2: return 3\n",
    "    elif action_count >= 1: return 2\n",
    "    else: return 1\n",
    "\n",
    "def evaluate_relevance(response):\n",
    "    \"\"\"Score 1-5 based on relevance to current learning scenario\"\"\"\n",
    "    # Look for scenario-specific terms\n",
    "    scenario_terms = [\n",
    "        'prompt engineering',\n",
    "        'requirements',\n",
    "        'anthropic',\n",
    "        'claude',\n",
    "        'evaluation',\n",
    "        'api',\n",
    "        'consultant',\n",
    "        'developer',\n",
    "        'transition'\n",
    "    ]\n",
    "    \n",
    "    relevance_score = sum(1 for term in scenario_terms if term in response.lower())\n",
    "    \n",
    "    if relevance_score >= 6: return 5\n",
    "    elif relevance_score >= 4: return 4\n",
    "    elif relevance_score >= 3: return 3\n",
    "    elif relevance_score >= 2: return 2\n",
    "    else: return 1\n",
    "\n",
    "def evaluate_structure(response):\n",
    "    \"\"\"Score 1-5 based on organization and readability\"\"\"\n",
    "    # Count structured elements\n",
    "    headers = len(re.findall(r'^#+\\s|^\\*\\*.*\\*\\*', response, re.MULTILINE))\n",
    "    lists = len(re.findall(r'^[-*]\\s|^\\d+\\.\\s', response, re.MULTILINE))\n",
    "    sections = len(re.findall(r'\\n\\n', response))\n",
    "    \n",
    "    structure_score = headers + min(lists // 3, 3) + min(sections // 2, 2)\n",
    "    \n",
    "    if structure_score >= 8: return 5\n",
    "    elif structure_score >= 6: return 4\n",
    "    elif structure_score >= 4: return 3\n",
    "    elif structure_score >= 2: return 2\n",
    "    else: return 1\n",
    "\n",
    "def evaluate_practicality(response):\n",
    "    \"\"\"Score 1-5 based on practical implementability\"\"\"\n",
    "    # Look for time-bounded, realistic suggestions\n",
    "    practical_indicators = [\n",
    "        r'daily',\n",
    "        r'week',\n",
    "        r'30 minutes',\n",
    "        r'hour',\n",
    "        r'small steps',\n",
    "        r'incrementally',\n",
    "        r'gradually'\n",
    "    ]\n",
    "    \n",
    "    practical_count = sum(len(re.findall(pattern, response.lower())) for pattern in practical_indicators)\n",
    "    \n",
    "    if practical_count >= 4: return 5\n",
    "    elif practical_count >= 3: return 4\n",
    "    elif practical_count >= 2: return 3\n",
    "    elif practical_count >= 1: return 2\n",
    "    else: return 1\n",
    "\n",
    "def evaluate_response(response):\n",
    "    \"\"\"Comprehensive evaluation of a prompt response\"\"\"\n",
    "    scores = {\n",
    "        'specificity': evaluate_specificity(response),\n",
    "        'actionability': evaluate_actionability(response),\n",
    "        'relevance': evaluate_relevance(response),\n",
    "        'structure': evaluate_structure(response),\n",
    "        'practicality': evaluate_practicality(response)\n",
    "    }\n",
    "    \n",
    "    scores['total'] = sum(scores.values())\n",
    "    scores['percentage'] = round((scores['total'] / 25) * 100, 1)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "print(\"‚úÖ Evaluation functions ready\")\nprint(\"üìä Scoring: Each criterion 1-5, total possible: 25 points\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if results_df is not None:\n",
    "    # Evaluate all prompt responses\n",
    "    evaluations = []\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        scores = evaluate_response(row['response'])\n",
    "        \n",
    "        evaluation = {\n",
    "            'version': row['version'],\n",
    "            'timestamp': row['timestamp'],\n",
    "            'token_count': row['token_count'],\n",
    "            **scores\n",
    "        }\n",
    "        \n",
    "        evaluations.append(evaluation)\n",
    "        \n",
    "        print(f\"\\nüß™ {row['version']} Evaluation:\")\n",
    "        print(f\"  Specificity: {scores['specificity']}/5\")\n",
    "        print(f\"  Actionability: {scores['actionability']}/5\")\n",
    "        print(f\"  Relevance: {scores['relevance']}/5\")\n",
    "        print(f\"  Structure: {scores['structure']}/5\")\n",
    "        print(f\"  Practicality: {scores['practicality']}/5\")\n",
    "        print(f\"  üìä Total: {scores['total']}/25 ({scores['percentage']}%)\")\n",
    "    \n",
    "    # Create evaluation DataFrame\n",
    "    eval_df = pd.DataFrame(evaluations)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ EVALUATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Sort by total score\n",
    "    eval_df_sorted = eval_df.sort_values('total', ascending=False)\n",
    "    \n",
    "    for idx, row in eval_df_sorted.iterrows():\n",
    "        print(f\"\\n{row['version']}: {row['total']}/25 ({row['percentage']}%)\")\n",
    "        print(f\"  Best areas: \", end=\"\")\n",
    "        \n",
    "        # Find top scoring areas\n",
    "        criteria_scores = {\n",
    "            'Specificity': row['specificity'],\n",
    "            'Actionability': row['actionability'],\n",
    "            'Relevance': row['relevance'],\n",
    "            'Structure': row['structure'],\n",
    "            'Practicality': row['practicality']\n",
    "        }\n",
    "        \n",
    "        top_areas = sorted(criteria_scores.items(), key=lambda x: x[1], reverse=True)[:2]\n",
    "        print(\", \".join([f\"{area} ({score}/5)\" for area, score in top_areas]))\n",
    "    \n",
    "    # Save evaluation results\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    eval_filename = f'data/evaluation_results_{timestamp}.csv'\n",
    "    eval_df.to_csv(eval_filename, index=False)\n",
    "    \n",
    "    print(f\"\\nüíæ Evaluation results saved to: {eval_filename}\")\n",
    "else:\n",
    "    print(\"‚ùå No data to evaluate. Run the prompts notebook first.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if 'eval_df' in locals() and not eval_df.empty:\n",
    "    print(\"üìà PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nüìä Overall Statistics:\")\n",
    "    print(f\"  Average score: {eval_df['total'].mean():.1f}/25 ({eval_df['percentage'].mean():.1f}%)\")\n",
    "    print(f\"  Best score: {eval_df['total'].max()}/25 ({eval_df['percentage'].max()}%)\")\n",
    "    print(f\"  Score range: {eval_df['total'].max() - eval_df['total'].min()} points\")\n",
    "    \n",
    "    # Criteria analysis\n",
    "    criteria = ['specificity', 'actionability', 'relevance', 'structure', 'practicality']\n",
    "    print(f\"\\nüéØ Criteria Performance:\")\n",
    "    \n",
    "    for criterion in criteria:\n",
    "        avg_score = eval_df[criterion].mean()\n",
    "        print(f\"  {criterion.capitalize()}: {avg_score:.1f}/5 avg\")\n",
    "    \n",
    "    # Best performing prompt\n",
    "    best_prompt = eval_df.loc[eval_df['total'].idxmax()]\n",
    "    print(f\"\\nüèÜ Best Performing Prompt: {best_prompt['version']}\")\n",
    "    print(f\"  Score: {best_prompt['total']}/25 ({best_prompt['percentage']}%)\")\n",
    "    print(f\"  Strengths: Highest scores in multiple criteria\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° Recommendations:\")\n",
    "    if eval_df['total'].max() >= 20:\n",
    "        print(f\"  ‚úÖ Strong performance - {best_prompt['version']} ready for deployment\")\n",
    "    elif eval_df['total'].max() >= 15:\n",
    "        print(f\"  üîß Good foundation - minor refinements needed\")\n",
    "    else:\n",
    "        print(f\"  üöß Needs improvement - consider major prompt revisions\")\n",
    "    \n",
    "    # Identify weakest areas for improvement\n",
    "    weakest_criterion = min(criteria, key=lambda x: eval_df[x].mean())\n",
    "    print(f\"  üéØ Focus improvement on: {weakest_criterion} (lowest average score)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No evaluation data available for analysis\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if 'eval_df' in locals() and not eval_df.empty:\n",
    "    # Create summary report\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    summary_report = f\"\"\"\n",
    "# Prompt Evaluation Summary Report\n",
    "Generated: {timestamp}\n",
    "\n",
    "## Test Results\n",
    "- Total prompts tested: {len(eval_df)}\n",
    "- Average score: {eval_df['total'].mean():.1f}/25 ({eval_df['percentage'].mean():.1f}%)\n",
    "- Best performing: {eval_df.loc[eval_df['total'].idxmax(), 'version']} ({eval_df['total'].max()}/25)\n",
    "\n",
    "## Ranking\n",
    "\"\"\"\n",
    "    \n",
    "    # Add ranking details\n",
    "    for i, (idx, row) in enumerate(eval_df.sort_values('total', ascending=False).iterrows(), 1):\n",
    "        summary_report += f\"{i}. {row['version']}: {row['total']}/25 ({row['percentage']}%)\\n\"\n",
    "    \n",
    "    summary_report += f\"\"\"\n",
    "## Next Actions\n",
    "- Deploy best performing prompt: {eval_df.loc[eval_df['total'].idxmax(), 'version']}\n",
    "- Use for actual learning requirements gathering\n",
    "- Continue A/B testing with refined versions\n",
    "\n",
    "## Data Files\n",
    "- Raw results: {csv_filename if 'csv_filename' in locals() else 'prompt_test_results_*.csv'}\n",
    "- Evaluations: {eval_filename if 'eval_filename' in locals() else 'evaluation_results_*.csv'}\n",
    "\"\"\"\n",
    "    \n",
    "    # Save summary report\n",
    "    report_filename = f'data/evaluation_summary_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.md'\n",
    "    with open(report_filename, 'w') as f:\n",
    "        f.write(summary_report)\n",
    "    \n",
    "    print(f\"\\nüìã Summary report saved to: {report_filename}\")\n",
    "    print(f\"\\n‚úÖ Evaluation complete! Check data/ folder for all results.\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot generate summary - no evaluation data available\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready for Next Phase\n",
    "\n",
    "This evaluation framework provides:\n",
    "\n",
    "‚úÖ **Systematic scoring** of prompt effectiveness\n",
    "‚úÖ **Data-driven comparison** between prompt versions  \n",
    "‚úÖ **Clear recommendations** for which prompts to deploy\n",
    "‚úÖ **Performance tracking** over time\n",
    "‚úÖ **Structured data** for further analysis\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run this evaluation after each prompt testing session\n",
    "2. Use results to refine prompts systematically\n",
    "3. Deploy best-performing prompts for actual requirements gathering\n",
    "4. Build historical performance database for continuous improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}