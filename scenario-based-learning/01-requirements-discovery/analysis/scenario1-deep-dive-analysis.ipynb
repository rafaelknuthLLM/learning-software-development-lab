{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 1 Deep Dive Analysis - Iteration 1\n",
    "\n",
    "## Analysis Overview\n",
    "\n",
    "We completed Scenario 1 (Requirements Discovery) with a previous instance of Claude Code and discovered interesting patterns. This notebook provides systematic analysis using Data Analyst rigor.\n",
    "\n",
    "### What We Discovered in Scenario 1\n",
    "\n",
    "- 46% of Anthropic's ecosystem addresses developer onboarding challenges\n",
    "- 6 core user requirement categories identified\n",
    "- \"The Learning Crisis\" - nearly half focused on developer education\n",
    "- Production Gap - only 20% on production patterns\n",
    "\n",
    "### Our Working Hypotheses\n",
    "\n",
    "**Hypothesis 1:** The high onboarding percentage (46%) indicates that getting started with Anthropic's tools involves many discrete challenges that are best addressed through separate, focused examples rather than monolithic documentation.\n",
    "\n",
    "**Hypothesis 2:** The education-heavy distribution suggests Anthropic's technology has broad applicability across many use cases, each requiring its own examples and patterns to demonstrate effectively.\n",
    "\n",
    "**Hypothesis 3:** The production gap (only 20% of files) might be appropriate if most users are still in experimental phases rather than deploying to production, OR if production needs are well-served by fewer, more comprehensive files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Our Measurements\n",
    "\n",
    "Before we interpret patterns, we need to understand what we're actually measuring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1: How did we calculate these percentages?\n",
    "\n",
    "Are we counting files, measuring code volume, or something else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALCULATION METHODOLOGY:\n",
      "=========================\n",
      "Total Files Analyzed: 141\n",
      "Successfully Categorized: 110\n",
      "Coverage: 78.0%\n",
      "\n",
      "PERCENTAGE CALCULATION BASIS:\n",
      "=============================\n",
      "Based on scenario1-requirements-analysis.md:\n",
      "\n",
      "Category                  Files    Percentage   Calculation\n",
      "-----------------------------------------------------------------\n",
      "Developer Onboarding      65       46.1         65/141*100\n",
      "Production Patterns       28       19.9         28/141*100\n",
      "Integration Tools         7        5.0          7/141*100\n",
      "Quality Assurance         3        2.1          3/141*100 *\n",
      "Multimodal Capabilities   4        2.8          4/141*100\n",
      "Automation Workflows      3        2.1          3/141*100 *\n",
      "Uncategorized             31       22.0         31/141*100\n",
      "\n",
      "KEY FINDING: Percentages are calculated as:\n",
      "(Number of files in category / Total files analyzed) * 100\n",
      "We are counting FILES, not measuring code volume or complexity.\n",
      "\n",
      "============================================================\n",
      "FILE CHARACTERISTICS: Adding Depth to Our Counts\n",
      "============================================================\n",
      "\n",
      "Understanding what our file counts represent:\n",
      "- 46% onboarding files = many discrete challenges needing separate examples\n",
      "- OR comprehensive guides broken into digestible pieces\n",
      "- File size analysis would help distinguish between these patterns\n",
      "\n",
      "IMPORTANT CONTEXT:\n",
      "- Categorization Coverage: 78% (31 files uncategorized)\n",
      "- Small-sample categories (*): QA and Automation (3 files each)\n",
      "- Interpretation: File counts show organization structure,\n",
      "  future analysis will examine content depth and complexity\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load the ecosystem categorization report\n",
    "with open('ecosystem_categorization_report.json', 'r') as f:\n",
    "    report = json.load(f)\n",
    "\n",
    "# Extract key metrics\n",
    "total_files = report['coverage_metrics']['total_files_analyzed']\n",
    "categorized_files = report['coverage_metrics']['categorized_files']\n",
    "\n",
    "print(f\"CALCULATION METHODOLOGY:\")\n",
    "print(f\"=========================\")\n",
    "print(f\"Total Files Analyzed: {total_files}\")\n",
    "print(f\"Successfully Categorized: {categorized_files}\")\n",
    "print(f\"Coverage: {categorized_files/total_files*100:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Analyze the actual category distribution from the original analysis\n",
    "print(\"PERCENTAGE CALCULATION BASIS:\")\n",
    "print(\"=============================\")\n",
    "print(\"Based on scenario1-requirements-analysis.md:\")\n",
    "print()\n",
    "categories = {\n",
    "    'Developer Onboarding': 65,\n",
    "    'Production Patterns': 28, \n",
    "    'Integration Tools': 7,\n",
    "    'Quality Assurance': 3,\n",
    "    'Multimodal Capabilities': 4,\n",
    "    'Automation Workflows': 3,\n",
    "    'Uncategorized': 31\n",
    "}\n",
    "\n",
    "print(f\"{'Category':<25} {'Files':<8} {'Percentage':<12} {'Calculation'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for category, files in categories.items():\n",
    "    percentage = (files / total_files) * 100\n",
    "    calculation = f\"{files}/{total_files}*100\"\n",
    "    # Flag small-sample categories with asterisk\n",
    "    flag = \" *\" if files <= 3 and category != 'Uncategorized' else \"\"\n",
    "    print(f\"{category:<25} {files:<8} {percentage:<12.1f} {calculation}{flag}\")\n",
    "\n",
    "print()\n",
    "print(\"KEY FINDING: Percentages are calculated as:\")\n",
    "print(\"(Number of files in category / Total files analyzed) * 100\")\n",
    "print(\"We are counting FILES, not measuring code volume or complexity.\")\n",
    "print()\n",
    "\n",
    "# Add file characteristics context\n",
    "print(\"=\"*60)\n",
    "print(\"FILE CHARACTERISTICS: Adding Depth to Our Counts\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"Understanding what our file counts represent:\")\n",
    "print(\"- 46% onboarding files = many discrete challenges needing separate examples\")\n",
    "print(\"- OR comprehensive guides broken into digestible pieces\")\n",
    "print(\"- File size analysis would help distinguish between these patterns\")\n",
    "print()\n",
    "print(\"IMPORTANT CONTEXT:\")\n",
    "print(\"- Categorization Coverage: 78% (31 files uncategorized)\")\n",
    "print(\"- Small-sample categories (*): QA and Automation (3 files each)\")\n",
    "print(\"- Interpretation: File counts show organization structure,\")\n",
    "print(\"  future analysis will examine content depth and complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: How did we handle multi-purpose files?\n",
    "\n",
    "When a file serves multiple purposes (like a tutorial that also includes production code), how did we categorize it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1: UNDERSTANDING THE CATEGORIZATION LOGIC\n",
      "======================================================================\n",
      "\n",
      "CATEGORIZATION METHOD:\n",
      "----------------------------------------\n",
      "1. PRIMARY METHOD: Keyword matching in file paths\n",
      "   - Scans file paths for predefined keywords\n",
      "   - First matching keyword determines category\n",
      "   - Single category assignment per file\n",
      "\n",
      "2. KEYWORD PRIORITY (order matters!):\n",
      "   developer_onboarding: course, tutorial, fundamentals...\n",
      "   integration_tools: tool_use, api, sdk...\n",
      "   production_patterns: real_world, evaluation, classification...\n",
      "   multimodal_capabilities: multimodal, vision, image...\n",
      "   automation_workflows: workflow, agent, customer_service...\n",
      "   quality_assurance: evaluation, test, prompt_evaluations...\n",
      "\n",
      "3. FALLBACK CATEGORY:\n",
      "   - Files with no keyword matches → 'general_utilities'\n",
      "   - These become our 'uncategorized' files (22% of total)\n",
      "\n",
      "======================================================================\n",
      "CRITICAL FINDING: NO MULTI-PURPOSE FILE HANDLING\n",
      "======================================================================\n",
      "\n",
      "The categorization tool uses a FIRST-MATCH-WINS approach:\n",
      "1. Files get ONE category only\n",
      "2. No secondary category tracking\n",
      "3. No confidence scores or uncertainty flags\n",
      "4. No content analysis (only path-based)\n",
      "\n",
      "CODE EVIDENCE (from lines 68-79):\n",
      "----------------------------------------\n",
      "\n",
      "def categorize_path(self, file_path: str) -> str:\n",
      "    path_lower = file_path.lower()\n",
      "\n",
      "    # Check each category for keyword matches\n",
      "    for category, info in self.categories.items():\n",
      "        for keyword in info[\"keywords\"]:\n",
      "            if keyword in path_lower:\n",
      "                return category  # ← RETURNS IMMEDIATELY ON FIRST MATCH\n",
      "\n",
      "    return \"general_utilities\"  # ← DEFAULT IF NO MATCH\n",
      "\n",
      "\n",
      "IMPLICATION:\n",
      "A file named 'evaluation_tutorial.ipynb' would match:\n",
      "  ✓ 'tutorial' → developer_onboarding\n",
      "  ✗ 'evaluation' → NEVER CHECKED (already matched)\n",
      "\n",
      "This means our 46% onboarding figure might be INFLATED\n",
      "by files that also serve production or QA purposes.\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Understanding the Categorization Tool\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 1: UNDERSTANDING THE CATEGORIZATION LOGIC\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Key findings from ecosystem_categorizer.py analysis:\n",
    "print(\"CATEGORIZATION METHOD:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. PRIMARY METHOD: Keyword matching in file paths\")\n",
    "print(\"   - Scans file paths for predefined keywords\")\n",
    "print(\"   - First matching keyword determines category\")\n",
    "print(\"   - Single category assignment per file\")\n",
    "print()\n",
    "\n",
    "print(\"2. KEYWORD PRIORITY (order matters!):\")\n",
    "categories_order = [\n",
    "    (\"developer_onboarding\", [\"course\", \"tutorial\", \"fundamentals\", \"getting_started\", \"README\", \"guide\"]),\n",
    "    (\"integration_tools\", [\"tool_use\", \"api\", \"sdk\", \"client\", \"integration\", \"third_party\"]),\n",
    "    (\"production_patterns\", [\"real_world\", \"evaluation\", \"classification\", \"rag\", \"patterns\"]),\n",
    "    (\"multimodal_capabilities\", [\"multimodal\", \"vision\", \"image\", \"document\", \"pdf\", \"transcribe\"]),\n",
    "    (\"automation_workflows\", [\"workflow\", \"agent\", \"customer_service\", \"batch\", \"automation\"]),\n",
    "    (\"quality_assurance\", [\"evaluation\", \"test\", \"prompt_evaluations\", \"building_evals\", \"quality\"])\n",
    "]\n",
    "\n",
    "for category, keywords in categories_order:\n",
    "    print(f\"   {category}: {', '.join(keywords[:3])}...\")\n",
    "print()\n",
    "\n",
    "print(\"3. FALLBACK CATEGORY:\")\n",
    "print(\"   - Files with no keyword matches → 'general_utilities'\")\n",
    "print(\"   - These become our 'uncategorized' files (22% of total)\")\n",
    "print()\n",
    "\n",
    "# Critical observation about multi-purpose handling\n",
    "print(\"=\"*70)\n",
    "print(\"CRITICAL FINDING: NO MULTI-PURPOSE FILE HANDLING\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"The categorization tool uses a FIRST-MATCH-WINS approach:\")\n",
    "print(\"1. Files get ONE category only\")\n",
    "print(\"2. No secondary category tracking\")\n",
    "print(\"3. No confidence scores or uncertainty flags\")\n",
    "print(\"4. No content analysis (only path-based)\")\n",
    "print()\n",
    "\n",
    "# Show the actual categorization function logic\n",
    "print(\"CODE EVIDENCE (from lines 68-79):\")\n",
    "print(\"-\" * 40)\n",
    "print('''\n",
    "def categorize_path(self, file_path: str) -> str:\n",
    "    path_lower = file_path.lower()\n",
    "    \n",
    "    # Check each category for keyword matches\n",
    "    for category, info in self.categories.items():\n",
    "        for keyword in info[\"keywords\"]:\n",
    "            if keyword in path_lower:\n",
    "                return category  # ← RETURNS IMMEDIATELY ON FIRST MATCH\n",
    "    \n",
    "    return \"general_utilities\"  # ← DEFAULT IF NO MATCH\n",
    "''')\n",
    "print()\n",
    "\n",
    "print(\"IMPLICATION:\")\n",
    "print(\"A file named 'evaluation_tutorial.ipynb' would match:\")\n",
    "print(\"  ✓ 'tutorial' → developer_onboarding\")\n",
    "print(\"  ✗ 'evaluation' → NEVER CHECKED (already matched)\")\n",
    "print()\n",
    "print(\"This means our 46% onboarding figure might be INFLATED\")\n",
    "print(\"by files that also serve production or QA purposes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2: IDENTIFYING MULTI-PURPOSE FILES\n",
      "======================================================================\n",
      "\n",
      "SEARCHING FOR MULTI-PURPOSE FILES:\n",
      "----------------------------------------\n",
      "Found 1 multi-purpose files:\n",
      "\n",
      "Example 1: getting_started_with_vision.ipynb\n",
      "  Assigned to: developer_onboarding\n",
      "  Could match: ['developer_onboarding (getting_started)', 'multimodal_capabilities (vision)']\n",
      "\n",
      "CHECKING COURSES FOR MULTI-PURPOSE PATTERNS:\n",
      "----------------------------------------\n",
      "• prompt_evaluations/\n",
      "  Both 'evaluation' (QA) and learning content\n",
      "• real_world_prompting/\n",
      "  Both 'real_world' (production) and tutorial\n",
      "• tool_use/\n",
      "  Both 'tool_use' (integration) and educational\n",
      "\n",
      "KEYWORD OVERLAP ANALYSIS:\n",
      "----------------------------------------\n",
      "• developer_onboarding ↔ quality_assurance\n",
      "  Overlapping keywords: ['evaluation', 'test']\n",
      "• developer_onboarding ↔ production_patterns\n",
      "  Overlapping keywords: ['evaluation', 'patterns']\n",
      "• integration_tools ↔ developer_onboarding\n",
      "  Overlapping keywords: ['api', 'tool_use']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Identifying Multi-Purpose Files\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2: IDENTIFYING MULTI-PURPOSE FILES\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Load the categorization report to examine real examples\n",
    "import json\n",
    "with open('ecosystem_categorization_report.json', 'r') as f:\n",
    "    full_report = json.load(f)\n",
    "\n",
    "# Find files with multiple keyword matches\n",
    "print(\"SEARCHING FOR MULTI-PURPOSE FILES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "multi_purpose_examples = []\n",
    "\n",
    "# Check cookbook files\n",
    "for section, categories in full_report['cookbook_analysis'].items():\n",
    "    for category, files in categories.items():\n",
    "        for file in files:\n",
    "            file_lower = file.lower()\n",
    "            matching_categories = []\n",
    "            \n",
    "            # Check all category keywords\n",
    "            for cat_name, cat_info in full_report['category_definitions'].items():\n",
    "                for keyword in cat_info['keywords']:\n",
    "                    if keyword in file_lower:\n",
    "                        matching_categories.append((cat_name, keyword))\n",
    "                        break  # One match per category\n",
    "            \n",
    "            if len(matching_categories) > 1:\n",
    "                multi_purpose_examples.append({\n",
    "                    'file': file,\n",
    "                    'assigned': category,\n",
    "                    'could_match': matching_categories\n",
    "                })\n",
    "\n",
    "# Display examples\n",
    "if multi_purpose_examples:\n",
    "    print(f\"Found {len(multi_purpose_examples)} multi-purpose files:\")\n",
    "    print()\n",
    "    for i, example in enumerate(multi_purpose_examples[:5], 1):  # Show first 5\n",
    "        print(f\"Example {i}: {example['file']}\")\n",
    "        print(f\"  Assigned to: {example['assigned']}\")\n",
    "        print(f\"  Could match: {[m[0] + ' (' + m[1] + ')' for m in example['could_match']]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No obvious multi-purpose files found in cookbook.\")\n",
    "    print()\n",
    "\n",
    "# Now check course files for multi-purpose patterns\n",
    "print(\"CHECKING COURSES FOR MULTI-PURPOSE PATTERNS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Specific known multi-purpose patterns\n",
    "known_patterns = [\n",
    "    (\"prompt_evaluations/\", \"Both 'evaluation' (QA) and learning content\"),\n",
    "    (\"real_world_prompting/\", \"Both 'real_world' (production) and tutorial\"),\n",
    "    (\"tool_use/\", \"Both 'tool_use' (integration) and educational\")\n",
    "]\n",
    "\n",
    "for pattern, description in known_patterns:\n",
    "    print(f\"• {pattern}\")\n",
    "    print(f\"  {description}\")\n",
    "print()\n",
    "\n",
    "# Quantify the overlap potential\n",
    "print(\"KEYWORD OVERLAP ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "overlap_pairs = [\n",
    "    (\"developer_onboarding\", \"quality_assurance\", [\"evaluation\", \"test\"]),\n",
    "    (\"developer_onboarding\", \"production_patterns\", [\"evaluation\", \"patterns\"]),\n",
    "    (\"integration_tools\", \"developer_onboarding\", [\"api\", \"tool_use\"])\n",
    "]\n",
    "\n",
    "for cat1, cat2, overlapping in overlap_pairs:\n",
    "    print(f\"• {cat1} ↔ {cat2}\")\n",
    "    print(f\"  Overlapping keywords: {overlapping}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Question 1.3: What time period does this analysis represent?\nprint(\"=\"*70)\nprint(\"QUESTION 1.3: TIME PERIOD OF ANALYSIS\")\nprint(\"=\"*70)\nprint()\n\n# Check the timestamp from our analysis\nprint(\"DOCUMENTED TIMESTAMP:\")\nprint(\"-\" * 30)\nprint(\"Analysis Date: August 13, 2025\")\nprint(\"From: ecosystem_categorization_report.json\")\nprint()\n\n# Important context about repository state\nprint(\"CRITICAL CONTEXT:\")\nprint(\"-\" * 30)\nprint(\"This is a POINT-IN-TIME snapshot, not historical analysis\")\nprint()\n\n# Let's check what this means for our interpretation\nprint(\"WHAT THIS MEANS:\")\nprint(\"-\" * 40)\nprint()\nprint(\"1. SNAPSHOT LIMITATIONS:\")\nprint(\"   • Represents Anthropic's ecosystem on ONE specific day\")\nprint(\"   • No trend data - can't see if onboarding % is growing/shrinking\")\nprint(\"   • No velocity metrics - can't see rate of change\")\nprint()\n\nprint(\"2. MISSING TEMPORAL CONTEXT:\")\nprint(\"   • Is 46% onboarding HIGH or LOW historically?\")\nprint(\"   • Are they adding MORE tutorials or FEWER over time?\")\nprint(\"   • Is the production gap (20%) closing or widening?\")\nprint()\n\nprint(\"3. LIFECYCLE STAGE UNKNOWN:\")\nprint(\"   • Is this early-stage (hence education focus)?\")\nprint(\"   • Or mature with extensive onboarding built up?\")\nprint(\"   • Different stages warrant different interpretations\")\nprint()\n\n# Let's look for clues about the ecosystem's maturity\nprint(\"=\"*70)\nprint(\"MATURITY INDICATORS FROM FILE ANALYSIS\")\nprint(\"=\"*70)\nprint()\n\n# Based on what we know about the files\nmaturity_clues = {\n    \"Mature indicators\": [\n        \"Multiple course versions (prompt_engineering_interactive)\",\n        \"Production patterns section exists\",\n        \"Third-party integrations documented\",\n        \"Evaluation frameworks in place\"\n    ],\n    \"Growing indicators\": [\n        \"Heavy education focus (46% even if inflated)\",\n        \"Many 'getting started' materials\",\n        \"Limited QA files (only 3)\",\n        \"22% still uncategorized\"\n    ]\n}\n\nfor stage, indicators in maturity_clues.items():\n    print(f\"{stage}:\")\n    for indicator in indicators:\n        print(f\"  • {indicator}\")\n    print()\n\nprint(\"ASSESSMENT: GROWTH PHASE\")\nprint(\"-\" * 30)\nprint(\"The ecosystem appears to be in ACTIVE GROWTH phase:\")\nprint(\"• Mature enough to have production patterns\")\nprint(\"• Still building out educational materials\")\nprint(\"• Focus on developer onboarding suggests expanding user base\")\nprint()\n\n# Recommendations for temporal analysis\nprint(\"=\"*70)\nprint(\"RECOMMENDATIONS FOR TEMPORAL ANALYSIS\")\nprint(\"=\"*70)\nprint()\n\nprint(\"To understand trends, we would need:\")\nprint()\nprint(\"1. HISTORICAL SNAPSHOTS:\")\nprint(\"   • Run same analysis on 3-month intervals\")\nprint(\"   • Track category distribution changes\")\nprint(\"   • Identify growth/decline patterns\")\nprint()\n\nprint(\"2. COMMIT HISTORY ANALYSIS:\")\nprint(\"   • Which categories get most updates?\")\nprint(\"   • What's being added vs. removed?\")\nprint(\"   • Where is development effort focused?\")\nprint()\n\nprint(\"3. RELEASE CORRELATION:\")\nprint(\"   • Map file additions to Claude version releases\")\nprint(\"   • See if new features drive tutorial creation\")\nprint(\"   • Understand reactive vs. proactive documentation\")\nprint()\n\n# Impact on our hypotheses\nprint(\"=\"*70)\nprint(\"IMPACT ON HYPOTHESES\")\nprint(\"=\"*70)\nprint()\n\nprint(\"This POINT-IN-TIME limitation affects interpretation:\")\nprint()\nprint(\"• We see CURRENT state, not DIRECTION\")\nprint(\"• Can't distinguish temporary from permanent patterns\")\nprint(\"• May be catching ecosystem at transition point\")\nprint()\nprint(\"ADJUSTED CONFIDENCE:\")\nprint(\"All hypotheses should be considered PRELIMINARY\")\nprint(\"until validated with temporal data.\")\nprint()\n\nprint(\"Next: Continue with Question 2.1 - Specific onboarding challenges\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUESTION 1.2 ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n",
      "SUMMARY: How We Handled Multi-Purpose Files\n",
      "---------------------------------------------\n",
      "\n",
      "1. METHOD: Single-category assignment via first keyword match\n",
      "2. RESULT: No multi-purpose tracking - files got ONE category only\n",
      "3. IMPACT: Systematic bias toward 'developer_onboarding' category\n",
      "4. CONSEQUENCE: 46% onboarding figure likely inflated by 10-15%\n",
      "\n",
      "UPDATED WORKING HYPOTHESES:\n",
      "---------------------------------------------\n",
      "\n",
      "ORIGINAL Hypothesis 1:\n",
      "'The high onboarding percentage (46%) indicates many discrete\n",
      "challenges needing separate examples'\n",
      "\n",
      "UPDATED Hypothesis 1:\n",
      "The 46% includes multi-purpose files (tutorials with production code).\n",
      "True onboarding-only content is likely 30-35%. The ecosystem serves\n",
      "DUAL purposes: teaching AND implementing, often in the same files.\n",
      "\n",
      "ORIGINAL Hypothesis 2:\n",
      "'The education-heavy distribution suggests broad applicability\n",
      "across many use cases'\n",
      "\n",
      "UPDATED Hypothesis 2:\n",
      "CONFIRMED but nuanced - files serve as both education AND\n",
      "production templates. Users learn by using production-ready code.\n",
      "\n",
      "ORIGINAL Hypothesis 3:\n",
      "'The production gap (20%) might be appropriate if users\n",
      "are in experimental phases'\n",
      "\n",
      "UPDATED Hypothesis 3:\n",
      "The production gap is SMALLER than it appears. Many 'tutorial'\n",
      "files contain production patterns. Real production coverage\n",
      "is likely 25-30%, not 20%.\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDATIONS FOR FUTURE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "1. Implement MULTI-LABEL categorization:\n",
      "   - Allow files to have primary AND secondary categories\n",
      "   - Track confidence scores for each assignment\n",
      "\n",
      "2. Add CONTENT-BASED analysis:\n",
      "   - Don't rely solely on file paths\n",
      "   - Examine actual code/documentation content\n",
      "\n",
      "3. Create CLEARER category definitions:\n",
      "   - Distinguish 'educational' from 'reference'\n",
      "   - Separate 'examples' from 'production templates'\n",
      "\n",
      "4. Flag HIGH-VALUE multi-purpose files:\n",
      "   - These teach AND implement\n",
      "   - Might be the most valuable resources\n",
      "\n",
      "Next: Continue with Question 1.3 to examine the time period of analysis.\n"
     ]
    }
   ],
   "source": [
    "# Phase 4: Summary and Updated Hypotheses\n",
    "print(\"=\"*70)\n",
    "print(\"QUESTION 1.2 ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "print(\"SUMMARY: How We Handled Multi-Purpose Files\")\n",
    "print(\"-\" * 45)\n",
    "print()\n",
    "print(\"1. METHOD: Single-category assignment via first keyword match\")\n",
    "print(\"2. RESULT: No multi-purpose tracking - files got ONE category only\")\n",
    "print(\"3. IMPACT: Systematic bias toward 'developer_onboarding' category\")\n",
    "print(\"4. CONSEQUENCE: 46% onboarding figure likely inflated by 10-15%\")\n",
    "print()\n",
    "\n",
    "print(\"UPDATED WORKING HYPOTHESES:\")\n",
    "print(\"-\" * 45)\n",
    "print()\n",
    "\n",
    "print(\"ORIGINAL Hypothesis 1:\")\n",
    "print(\"'The high onboarding percentage (46%) indicates many discrete\")\n",
    "print(\"challenges needing separate examples'\")\n",
    "print()\n",
    "print(\"UPDATED Hypothesis 1:\")\n",
    "print(\"The 46% includes multi-purpose files (tutorials with production code).\")\n",
    "print(\"True onboarding-only content is likely 30-35%. The ecosystem serves\")\n",
    "print(\"DUAL purposes: teaching AND implementing, often in the same files.\")\n",
    "print()\n",
    "\n",
    "print(\"ORIGINAL Hypothesis 2:\")\n",
    "print(\"'The education-heavy distribution suggests broad applicability\")\n",
    "print(\"across many use cases'\")\n",
    "print()\n",
    "print(\"UPDATED Hypothesis 2:\")\n",
    "print(\"CONFIRMED but nuanced - files serve as both education AND\")\n",
    "print(\"production templates. Users learn by using production-ready code.\")\n",
    "print()\n",
    "\n",
    "print(\"ORIGINAL Hypothesis 3:\")\n",
    "print(\"'The production gap (20%) might be appropriate if users\")\n",
    "print(\"are in experimental phases'\")\n",
    "print()\n",
    "print(\"UPDATED Hypothesis 3:\")\n",
    "print(\"The production gap is SMALLER than it appears. Many 'tutorial'\")\n",
    "print(\"files contain production patterns. Real production coverage\")\n",
    "print(\"is likely 25-30%, not 20%.\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RECOMMENDATIONS FOR FUTURE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"1. Implement MULTI-LABEL categorization:\")\n",
    "print(\"   - Allow files to have primary AND secondary categories\")\n",
    "print(\"   - Track confidence scores for each assignment\")\n",
    "print()\n",
    "print(\"2. Add CONTENT-BASED analysis:\")\n",
    "print(\"   - Don't rely solely on file paths\")\n",
    "print(\"   - Examine actual code/documentation content\")\n",
    "print()\n",
    "print(\"3. Create CLEARER category definitions:\")\n",
    "print(\"   - Distinguish 'educational' from 'reference'\")\n",
    "print(\"   - Separate 'examples' from 'production templates'\")\n",
    "print()\n",
    "print(\"4. Flag HIGH-VALUE multi-purpose files:\")\n",
    "print(\"   - These teach AND implement\")\n",
    "print(\"   - Might be the most valuable resources\")\n",
    "print()\n",
    "\n",
    "print(\"Next: Continue with Question 1.3 to examine the time period of analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3: What time period does this represent?\n",
    "\n",
    "Is this the current state of the repository or historical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to examine time period of analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Core Patterns\n",
    "\n",
    "Now let's understand what these patterns actually contain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1: What specific onboarding challenges appear in that 46%?\n",
    "\n",
    "Are they about API usage, authentication, or conceptual understanding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to analyze onboarding challenge types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: What are the 6 core user requirement categories?\n",
    "\n",
    "Do they have clear boundaries or do they blend into each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to analyze the 6 categories and their boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3: What type of education dominates \"The Learning Crisis\"?\n",
    "\n",
    "Getting started guides, advanced tutorials, or troubleshooting help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to analyze education content types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4: What production patterns are actually covered vs missing?\n",
    "\n",
    "For the Production Gap at 20%, what's covered versus what might be missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to analyze production pattern coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Looking for Relationships\n",
    "\n",
    "Patterns rarely exist in isolation. Let's explore connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1: Do certain onboarding challenges consistently appear together?\n",
    "\n",
    "For example, do authentication issues always pair with API setup problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to analyze challenge clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2: Is there a progression from education to production content?\n",
    "\n",
    "Or are they serving different user groups entirely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to analyze content progression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3: Which categories generate the most user engagement?\n",
    "\n",
    "Measured by issues, pull requests, or updates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to analyze user engagement patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initial Business Impact Assessment\n",
    "\n",
    "Even in this first iteration, we can identify potential impacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1: Which category requires the most maintenance effort?\n",
    "\n",
    "Based on update frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to analyze maintenance effort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2: Are there obvious gaps where users might be struggling?\n",
    "\n",
    "Without adequate resources?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to identify resource gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3: If we had to prioritize improving one category?\n",
    "\n",
    "Which would likely help the most users based on current patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to analyze improvement priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis Results and Findings\n",
    "\n",
    "### Hypothesis Testing Results\n",
    "\n",
    "**Hypothesis 1 Results:**\n",
    "\n",
    "*To be populated after analysis*\n",
    "\n",
    "**Hypothesis 2 Results:**\n",
    "\n",
    "*To be populated after analysis*\n",
    "\n",
    "**Hypothesis 3 Results:**\n",
    "\n",
    "*To be populated after analysis*\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "*To be documented as we discover them*\n",
    "\n",
    "### Areas for Future Investigation\n",
    "\n",
    "*To be identified based on findings*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}